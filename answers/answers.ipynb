{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Приведите по 2 примера, когда лучше максимизировать Precision, а когда Recall.\n",
    "Precision: для пропуска в закрытые учреждения, для инициализации доступа банковских клиентов \n",
    "Recall: для поиска и спасения людей, для привлечения новых клиентов в бизнес,поиск опасных веществ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Почему мы используем F-меру, почему, например, нельзя просто взять среднее от Precision и Recall?\n",
    "Причина в том, что для того, чтобы среднее значение было действительным,нужно, чтобы значения были в одних\n",
    "и тех же масштабированных единицах. Precision и Recall имеют истинные положительные значения в числителе и разные знаменатели. \n",
    "Чтобы усреднить их, на самом деле имеет смысл только усреднить их взаимные отношения, таким образом, гармоническое среднее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3. В чём различие между зависимыми и независимыми выборками?\n",
    "Независимые выборки - сравниваются две разные группы, например мужчины и женщины, молодые и пожилые и т.д. \n",
    "Зависимые выборки как правило возникают, когда речь идет об одной группе испытуемых до и после эксперементального воздействия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.Когда применяются параметрические статистические критерии, а когда — их непараметрические аналоги?\n",
    "Согласно статистическим методам статистические критерии делятся на\n",
    "параметрические и непараметрические.\n",
    "Параметрические критерии используются в задачах проверки параметрических гипотез\n",
    "и включают в свой расчет показатели распределения, например, средние, дисперсии и т.д.\n",
    "Это такие известные классические критерии, как критерий Стьюдента, критерий Фишера\n",
    "и др. Они позволяют сравнить основные параметры генеральных совокупностей, а также\n",
    "оценить разности средних и различия в дисперсиях. Критерии способны выявить\n",
    "тенденции изменения признака, оценить взаимодействие двух и более факторов в\n",
    "воздействии на изменения признака\n",
    "Непараметрические критерии проверки гипотез основаны на операциях с другими\n",
    "данными, в частности, частотами, рангами и т.п. Это - критерий Манна-Уитни, критерий\n",
    "Уилкоксона и многие другие. Непараметрические критерии позволяют решить некоторые\n",
    "важные задачи, связанные с выявлением различий исследуемого признака, с оценкой\n",
    "сдвига значений исследуемого признака, выявлением различий в распределениях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?\n",
    "Микро- и макро-средние (для любой метрики) будут вычислять несколько разные вещи, и, следовательно, их интерпретация будет\n",
    "разной. Макро-среднее вычислит метрику независимо для каждого класса, а затем возьмет среднее (следовательно, будет обрабатывать \n",
    "все классы одинаково), тогда как микро-среднее будет агрегировать вклады всех классов для вычисления средней метрики. \n",
    "В мультиклассовой классификации предпочтение отдается микро-среднему, если вы подозреваете, что может быть дисбаланс классов \n",
    "(т.е. у вас может быть гораздо больше примеров одного класса, чем других классов).\n",
    "micro говорит, что функция для вычисления f1, учитывая общее количество истинных положительных, ложных отрицательных и ложных положительных результатов (независимо от прогноза для каждой метки в наборе данных)\n",
    "macro говорит, что функция вычисляет f1 для каждой метки, и возвращает среднее значение без учета пропорции для каждой метки в наборе данных.\n",
    "weighted говорит, что функция вычисляет f1 для каждой метки, и возвращает среднее значение с учетом пропорции для каждой метки \n",
    "в наборе данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?\n",
    "CatBoost — это библиотека градиентного бустинга, созданная Яндексом. Она использует небрежные (oblivious) деревья решений,\n",
    "чтобы вырастить сбалансированное дерево. Одни и те же функции используются для создания левых и правых разделений (split)\n",
    "на каждом уровне дерева. CatBoost позволяет использовать категориальные признаки без необходимости их предварительно обрабатывать.\n",
    "-CatBoost позволяет проводить обучение на нескольких GPU.\n",
    "-Библиотека позволяет получить отличные результаты с параметрами по умолчанию, что сокращает время, необходимое для настройки гиперпараметров.\n",
    "-Обеспечивает повышенную точность за счет уменьшения переобучения.\n",
    "-Возможность быстрого предсказания с применением модели CatBoost;\n",
    "-Обученные модели CatBoost можно экспортировать в Core ML для вывода на устройстве (iOS).\n",
    "-Умеет под капотом обрабатывать пропущенные значения.\n",
    "-Может использоваться для регрессионных и классификационных задач.\n",
    "В LightGBM нет структуры данных для узла. Вместо этого в структуре данных дерева Tree содержатся массивы значений, \n",
    "где в качестве индекса выступает номер узла. Значения в листьях также хранятся в отдельных массивах.\n",
    "XGBoost - это библиотека с открытым исходным кодом, обеспечивающая высокопроизводительную реализацию деревьев решений \n",
    "с градиентным усилением. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Расскажите, как работает регуляризация в решающих деревьях, какие параметры мы штрафуем в данных алгоритмах?\n",
    "Регуляризация - это методика, используемая для уменьшения ошибки путем надлежащего подбора функции в заданном обучающем \n",
    "наборе и предотвращения переоснащения.В семье моделей на основе деревьев принятия решений одно дерево способно выучить \n",
    "все данные. Это приводит к сильному переобучению. \n",
    "Глубина дерева — параметр, который ограничивает максимальный рост дерева (деревья принятия решений растут в глубину). \n",
    "Этот параметр позволяет уменьшить переобучение, но ограничивает количество переменных для каждого конкретного листа.\n",
    "Минимальный вес листа — параметр, который ограничивает рост дерева, когда следующее деление листа приводит к тому, \n",
    "что хотя бы в одном из них слишком мало наблюдений, что делало бы его слишком специфичным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8. По какому принципу рассчитывается \"важность признака (feature_importance)\" в ансамблях деревьев?\n",
    "Методы оценки важности feature_importances_, встроенными в алгоритмы построения ансамблей деревьев. Как правило, они основаны \n",
    "на вычислении суммарного уменьшения минимизируемого функционала ошибки с помощью ветвлений по рассматриваемому признаку. \n",
    "Нет идеального алгоритма оценки важности признаков (для любого можно подобрать пример, когда он плохо работает),\n",
    "если много похожих (например, сильно коррелированных признаков), то важность может «делиться между ними», \n",
    "поэтому не рекомендуется отбрасывать признаки по порогу важности, есть старая рекомендация (впрочем, без теоретического \n",
    "обоснования): модель для решения задачи и оценка важности должны основываться на разных парадигмах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
